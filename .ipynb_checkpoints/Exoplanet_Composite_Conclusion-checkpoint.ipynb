{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eae61364-710e-414a-b853-15333763a724",
   "metadata": {},
   "source": [
    "# Concluding the Exoplanet Composite Discovery Method Predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43396f69-15e9-4760-98e0-e68067ff221b",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebbe627-e720-4a30-8fbe-63e5703574ed",
   "metadata": {},
   "source": [
    "### In the preprocessing ipynb, I started by using pandas to read the raw csv, made a copy and displayed the raw csv info\n",
    "### Using a missing value threshold of 0, I removed columns that exceeded this threshold. 0 made the most sense as a threshold to simplify model training and reduce problem complexity. \n",
    "### After reducing the number of features, I used the exoplanet archive column mapping csv to map the table name (ex: pl_name) to the Description (ex: Planet name) using a dictionary\n",
    "### Used a for loop through the number of code columns (358) and their definitions, where if row i in the database column matches a column name in the filtered raw data, you assign the key and value to the dictionary, removing trailing and leading whitespaces\n",
    "### Used boolean to confirm we mapped all database column names to their definitions\n",
    "### Then just reassigned the raw data column values (the names) as the values of the dictionary {key:value}\n",
    "### Checkpoint where we create a copy of the current dataset we are working with\n",
    "### Then I checked unique values in the \"Discovery Method\" column in the dataset to get an idea of the discovery methods possible\n",
    "### Using value_count, I got an idea of the frequency of each of the discovery methods in the column. Transit was by far the most frequent\n",
    "### Some of the discovery methods only had 10 instances, so I used SMOTE (synthetic minority oversampling technique) to synthetically generate instances of the minority class for a better class balance. This is essential for machine learning models to properly learn how to predict the discovery methods and not just predict the most frequent discovery method. \n",
    "### Realized that if I encoded Discovery Facility, Discovery Telescope and Discovery Instrument, there are 70-90 unique values for EACH, so encoding these was determined unfeasable\n",
    "### So I dropped them along with Planet name and Host name, which would be irrelevant to model training\n",
    "### Then I removed all the one hot binary encoded variables \"Detected by...\" columns except for transits\n",
    "### This is because if \"Detected by Transits\" is 0, we already know that it has to be one of the other 10 discovery methods\n",
    "### Before doing that, I removed instances of exoplanets that were found by more than one discovery method for more straightforward model training\n",
    "### This was done by summmating each of the target dummies for each of the exoplanets, and if the sum was greater than 1 that means it was discovered by more than one method, warranting its removal\n",
    "### After checking the new observation count for each of the \"Detected by...\" methods, we dropped the unnecessary \"Detected by...\" columns\n",
    "### Checking the ratio of \"Detected by transits\" to the total number of instances, I saw it transits was roughly 71% of discovery method instances\n",
    "### Imported SMOTE and specified my features (all variables except \"Detected by Transits\") and targets (\"Detected by Transits\")\n",
    "### The new ratio was 50%, implying SMOTE worked as intended (50% exoplanets discovered by Transits, 50% were not)\n",
    "### Then proceeded with standardization; Removed columns that could not be standardized (Discovery Year, Circumbinary Flag, Controversial Flag) and added these after standardizing\n",
    "### Imported StandardScaler function from sklearn preprocessing library and fit the scaler to all the unscaled features.\n",
    "### Then applied scaler.transform to all the unscaled features besides the ones excluded above. Checked the shapes of features, targets and the variables we removed, they all must match\n",
    "### Since we used StandardScaler the scaled features are now in numpy array format. To combine it with the other excluded variables, it must first be convered to a pandas dataframe. Then we just added the excluded variables to the new pandas dataframe of the scaled features\n",
    "### Converted discovery year to be treated as a categorical variable for the purposes of this project\n",
    "### One last checkpoint (creating copy of the current dataframe we are working with) \n",
    "### Finally export the preprocessed dataframe and turn it into a csv to be saved in the current directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d106126e-e636-4d80-9730-15444fda80e4",
   "metadata": {},
   "source": [
    "## XGB Boost ALL Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee90ffc3-a515-4777-b6f9-b0421fb128df",
   "metadata": {},
   "source": [
    "### Why XGB?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb04f86d-a73e-4e97-80ac-6df25b8873d2",
   "metadata": {},
   "source": [
    "### After the initial preprocessing I got started with the first model I deployed for this analysis\n",
    "### XGBoost was a great first option because it is a powerful implementation of gradient boosting algorithms designed for tabular data\n",
    "#### The XGB captures complex patterns in data by combining predictions of multiple weak learners (typically decision trees)\n",
    "### XGB also provides clear metrics for feature importance, allowing enhanced understanding of which features most influences the model's predictions. This is useful to my task in predicting exoplanet discovery methods and finding out which features influenced the model to make these predictions \n",
    "### Additionally, XGB handles unbalanced data well. Although we addressed these imbalances in the preprocessing, it is still beneficial that XGB has several techniques to handle imbalanced data, such as scale_pos_weight to assign more weight to the less frequent classes to improve the model's ability to predict them\n",
    "### XGB is also very fast and high performing, as it handles sparse data, uses parallel processing and regularization techniques that prevent overfitting \n",
    "### XGB has flexibility with the feature types, including categorical and continuous features, without needing to extensively preprocess\n",
    "### Finally, XGBoost has been widely adopted in scientific fields like astronomy for its accuracy and robustness in both classification and regression tasks. It is very handy in managing non-linear relationships and interactions between features "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72727544-03f6-4ad6-9545-df428055b916",
   "metadata": {},
   "source": [
    "### By including all the features in this XGB model, I aimed to reduce the dimensionality of the problem by identifying which features weighed the most according to the model\n",
    "### Firstly imported the preprocessed data using pandas, and defined my train test split, as well as features and targets (features are all features except the target variable we are predicting, which is detected by transits)\n",
    "### Then, I observed feature correlations to observe if there was any moderate to high correlation between variables, which would interfere with model training and performance. Should I have found any notable correlations, that warranted removal of one of them\n",
    "### After this I proceeded to set a random seed for reproducability using random and numpy libraries. This means every time I set random state to 42 it shuffles the same RANDOM way\n",
    "### Then I actually split the data into training, testing and validation (80% training, 10% testing, 10% validation). This was achieved through 2 separate splits (one to get 80:20, then to get that 20 into 10:10)\n",
    "### Following the split, I used the XGBClassifier class to create my model from the xgb library, with logloss as the evaluation metric. logloss is appropriate because it is very suitable for binary classification problems\n",
    "### Since XGB cannot have \"[\" or \"]\" or \"<\" or \">\" characters in the feature names, I used the lambda function to replace these characters with empty spaces\n",
    "### Then I actually fit the model with the new clean training, and validation data, with early stopping = 10 so that if theres no improvement in 10 consecutive trials the model stops to prevent overfitting and to make the overall process more efficient\n",
    "### After fitting the model, I used .predict method on the cleaned test set, so I can see what predictions the model makes on each instance with my own eyes\n",
    "### Evaluation of performance metrics was done using classificaiton report, accuracy score, roc auc score, and a confusion matrix. The actual evaluation was a comparison between the model's predictions and the y_test (the actual values of whether or not this exoplanet was discovered by transits)\n",
    "### The XGB Model with all features had an accuracy of a whopping 96.42% on the test set, with an ROC AUC of 98.59%\n",
    "### Right after the evaluation metrics, I looked at the K fold cross validation score using sklearn the library, and declared 5 shuffled folds. I calculated the average cross validation accuracy (96.77%) and the average cross validation ROC AUC (99.3%)\n",
    "### I wanted to see which features were deemed important by the model so I made a feature importance table, where the importance type is the WEIGHT of the feature. Naturally this means that I wanted to see which features had the greater weight on the model. \n",
    "### After extracting the weights of each of the coefficients from the fitted xgboost model using .get_booster().get_score('weight'), and the feature names from the feature training data, I created a pandas dataframe with the Features and their corresponding importance \n",
    "### Firstly had to turn the feature_imporance items into a list, set the columns, and sorted the values based on Importance in descending order\n",
    "### I then plotted this feature importance table using matplot lib, and used xgb.plot_importance to directly plot from the xgb model \n",
    "### This plot demonstrates the F score. A higher F score indicates the feature is deemed important by the model\n",
    "### For the rest of my models, to reduce dimensionality, I only included features that had an F score of 30 or above. These being: \n",
    "### 4   Ecliptic Latitude deg\t271.0\n",
    "### 2\tGalactic Latitude deg\t250.0\n",
    "### 3\tGalactic Longitude deg\t223.0\n",
    "### 5\tEcliptic Longitude deg\t201.0\n",
    "### 11\tDiscovery Year\t132.0\n",
    "### 1\tNumber of Planets\t78.0\n",
    "### 6\tNumber of Photometry Time Series\t32.0\n",
    "### 0\tNumber of Stars\t20.0\n",
    "\n",
    "### With this new revised list of features, I had a better idea of the features I wanted to train the rest of my models with"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ff3d73-5b7f-4f12-b2bb-a76ee091de48",
   "metadata": {},
   "source": [
    "# XGBoost REFINED features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adaab5f-19ec-4cd5-b239-7202ca8698a8",
   "metadata": {},
   "source": [
    "## After training and testing the XGBoost with all features, I trained and tested a new XGBoost model with refined features. The purpose of this is to see if after reducing multi-dimensionality, model performance would be enhaced as the problem becomes less complex. \n",
    "## The first time around training and testing the new XGBoost, I had excluded \"Number of Stars,\" but found that model performance suffered every so slightly (test accuracy went down 0.3% from 96.42% -> 96.11%), ultimately prompting me to leave it in. \n",
    "## The ipynb starts off the same, using pandas to import and read the preprocessed composite csv\n",
    "## Then, I created a list full of the features I was going to exclude, and used the .drop method to remove them with axis=1 (1 is for columns, 0 is for rows. Makes sense because format is (rows, cols))\n",
    "## After this I defined my targets variable (detected by transits) aka the variable I want my model to predict, while features are everything else (did this by just dropping the targets variable)\n",
    "## Feature analysis includes looking at the correlations so I just used the .corr() method to get a table looking at correlationss between variables. For model training its rule of thumb to avoid multicollinearity. \n",
    "## I observed the following correlations:\n",
    "## Weak:\n",
    "### Ecliptic Latitude and Number of Photometry Time Series; -0.222\n",
    "### Number of Photometry Time Series and Discovery Year; -0.253\n",
    "\n",
    "\n",
    "## Moderate:\n",
    "### Galactic Latitude and Ecliptic Latitude; .463\n",
    "### Galactic Longitude and Ecliptic Latitude; -.657\n",
    "### Galactic Longitude and Ecliptic Longitude; -.408\n",
    "### Ecliptic Latitude and Ecliptic Longitude; .498\n",
    "\n",
    "\n",
    "## The rest of the feature pairs can be characterized as having very weak correlations\n",
    "\n",
    "## Since none of the feature correlations were excessive, I didn't remove any further variables\n",
    "## Following observing the feature correlations I set the random seed for 42 once again\n",
    "## I split the data into training, testing and validation in the same exact way as in the XGBoost model with all features (80:10:10)\n",
    "## The XGBoost was created essentially in the same way as in the ipynb with all features. logloss is still the best evaluation metric for this problem\n",
    "## Similar to the XGBoost with all features, removed incompatible characters from the features for training, testing and validation\n",
    "## Then fit the new XGBoost with early stopping rounds = 10 again\n",
    "## After fitting, I wanted to see the actual predictions of the model on the test set again, so i did this with .predict method on x_test_clean\n",
    "## Looked at the same performance metrics (classification report, accuracy score, roc auc score and confusion matrix) \n",
    "## The Refined XGboost model had the same accuracy on the test set: 96.42%, with a lower ROC of 98.57%. The reduction is so miniscule it can be considered insignificant. \n",
    "## The confusion matrices in both were also exactly the same\n",
    "## Same protocol for k-cross validation, but this time the refined model performed a bit worse at 96.67% and a mean CV ROC AUC of 99.25%, both reductions in performance insignificant. \n",
    "## The feature importance table looks exactly the same as in the XGBoost with all features, just with different F scores. Both feature tables had more or less the same order and importance ranking for the features, the only difference being galactic longitude scoring higher than galactic longitude in the XGBoost refined model. Again, this difference (2) is insignificant. \n",
    "## Both XGB models suggest that positional data (ecliptic and galactic coordinates) plays a critical role in detecting exoplanets by the transit method likely due to the necessity of specific orbital alignments. Discovery year highlights the importance of technological advancements in improving the detection capabilities of the transit method over time. \n",
    "\n",
    "### Ecliptic Latitude (deg)\n",
    "#### Ecliptic latitude is the position of the exoplanet relative to the ecliptic plane, which is the apparent path of the Sun on the celestial sphere. A high ranking of importance for feature suggests that the position of an exoplanet relative to this plane is crucial for detecting it via transits. This makes sense because transit detection is easier when the orbital plane of the exoplanet is aligned with the line of sight from Earth, which is more likely near the ecliptic plane.\n",
    "\n",
    "### Galactic Longitude (deg)\n",
    "#### Galactic longitude refers to the position of the exoplanet within the Milky Way. This feature's high importance indicatess that certain regions of the galaxy are more conducive to detecting exoplanets by transits, possibly due to the density of stars or observational strategies that focus on specific regions of the galaxy.\n",
    "\n",
    "### Galactic Latitude (deg)\n",
    "#### Similar to galactic longitude, galactic latitude describes the exoplanet's position relative to the galactic plane. The importance of this feature suggests that exoplanet detection via transits might be more successful in particular regions of the galaxy, such as near the galactic plane where star density is higher, thus increasing the likelihood of observing a transit.\n",
    "\n",
    "### Ecliptic Longitude (deg)\n",
    "#### Ecliptic longitude, like ecliptic latitude, is another coordinate that describes the position of the exoplanet in the sky. Its importance in the model implies that the specific positioning relative to the ecliptic plane has a significant impact on the ability to detect transits.\n",
    "\n",
    "### Discovery Year \n",
    "#### The discovery year being an important feature reflects the improvements in technology and observational techniques over time. As methods for detecting transits have advanced, the ability to discover exoplanets using this method has likely improved with it. \n",
    "\n",
    "### Context:\n",
    "#### Exoplanet Detection by Transits: The transit method relies on detecting the slight dimming of a star as an exoplanet passes in front of it. The effectiveness of this method is influenced by the alignment of the exoplanet's orbit with the observer's line of sight, which is why positional coordinates like ecliptic and galactic latitude/longitude are crucial.\n",
    "\n",
    "#### Technological Advances: The importance of \"Discovery Year\" suggests that as detection technology has improved, particularly with missions like Kepler and TESS, the ability to detect exoplanets using the transit method has also increased. This makes sense because of historical trends in exoplanet discovery, where a surge in transit detections followed the launch of these missions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1995cd16-fb9e-47f1-ba1d-e1a1e9c24a5d",
   "metadata": {},
   "source": [
    "# Logistic Regressison with Refined Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fc5b68-5d22-4b74-9e2a-04bdfa82e27b",
   "metadata": {},
   "source": [
    "## Next is the logistic regression. For this model, I used the same feature selection as in my XGBoost model with refined features, for a more straightforward comparison of model performance and logical consistency. \n",
    "## The ipynb starts off the same, using pandas to load and read the preprocessed composite csv\n",
    "## Declared the columns I want to drop in a list, and then used .drop axis =1 to remove them from the dataset\n",
    "## Before doing the training and testing split, I checked the Variance Inflation Factor between the variables; a measure of how much the variance of a regression coefficient is inflated due to multicollinearity with other predictors; A VIF > 5 indicates high multicollinearity\n",
    "## using statsmodels library to import variance_inflation_factor and add_constant, I defined my targets and features again\n",
    "## This time, I added a constant to my features to ensure that the model can fit an intercept, allowing for predictions even when all other feature values are 0, and to allow the design matrix to achieve full rank.\n",
    "## Creating a dataframe for visual comprehension, I created a Features column using the columns in the features_with_intercept variable, then I created a VIF column using a list comprehension for loop that calls the variance_inflation_factor method on all the values in the features_with_intercept subset, and i for each integer up to the shape of the features_with_intercept subset\n",
    "## The VIF table reveals that all the variables had a VIF less than or equal to 3. VIF values of 4 and 5 are moderate, so none of the features demonstrated moderate to high VIF and warranted removal. \n",
    "## Then I observed feature correlations, and set the random seed of 42 once again\n",
    "## After this, I split the data, but this time only into 80:20 (training: testing) instead of 80:10:10 (training, testing, validation) because I deployed Gridsearch with cross validation, which essentially handles the validation portion of our training\n",
    "## Following importing the sklearn libraries, I used the LogisticRegression class and sklearn metrics for model evaluation\n",
    "## Expanding a bit on hyperparameter tuning, Gridssearch automates finding the optimal hyperparameter process all while validating the model's performance\n",
    "## The hyperparameters to choose from included penalty (lasso regularization or ridge), C (inverse of regularization strength) where smaller values of C imply stronger regularization to prevent overfitting and larger values allow the model to fit the training data more closely, Solver (liblinear is best for smaller datasets and supports l1 and l2 regularization), and max number of iterations \n",
    "## After creating an instance of the LogisticRegression class and storing it in a variable with random_state= 42, I defined my parameter grid with a dictionary that contains all the hyperparameter values I want GridSearch to try and compare. \n",
    "## I created an instance of the GridSearch class and tossed in my logistic regression, parameter grid dictionary, number of cross valdations and accuracy scoring as parameters\n",
    "## Then I fit the grid search object to x train and y train\n",
    "## After letting GridSearch look for the optimal parameters, I am able to extract the best parameters using the best_params_ function, and the best model using .best_estimator_\n",
    "## The best hyperparameters as determined by GridSearch were the following:\n",
    "### Best hyper parameters: {'C': 0.1, 'max_iter': 500, 'penalty': 'l1', 'solver': 'liblinear'}\n",
    "## Now that I have the best logistic regression model I am ready to deply the model to make predictions on the test set and store the predictions in a variable \n",
    "## After the predictions, I looked at the performance metrics (classification report, accuracy score, roc auc score, and confusion matrix) \n",
    "## These performance metrics revealed the logistic regression model with the optimal hyperparameters has a 85.25% accuracy on the test set, with a 92.66% roc auc score on the test set. Although not better than the XGBoost models, the logistic regression performed decently. \n",
    "## Then I decided to create a summary table with coefficients (weights) and intercept (bias) to get an idea of which variables were important to the model \n",
    "## After using .intercept_ and .coef_ methods to derive the bias and weights of each of the variables and matching up coefficients with the features, I transposed the coefficients numpy array to be interpreted as columns instead of rows (the default)\n",
    "## Finally, I inserted the intercept in the 0th index and moved all indices up by 1\n",
    "## Now that I have the bias and weights lined up, I decided to calculate log odds of each of the variables for a more enhanced understanding of their relationship with the target variable. Log odds can be calculated by taking the natural log of each of the weights e^ x where x is the weight\n",
    "## A feature is deemed not important if its coefficient is near 0, and if logodds are near 1 (because e^ 0 is 1)\n",
    "## The feature summary table reveals the following features as important: \n",
    "###     Feature Name  |  Coefficients  |  Odds_ratio\n",
    "### 5\tEcliptic Latitude [deg]  |  1.691207  |  5.426025 *** \n",
    "### 2\tNumber of Planets  |  0.426255  |  1.531511\n",
    "### 6\tEcliptic Longitude [deg]  |  0.424376  |  1.528636\n",
    "### 4\tGalactic Longitude [deg]  |  0.347762  |  1.415895\n",
    "### The following variables had a negative weight and log odds less than 1\n",
    "### Galactic Latitude [deg]\t-0.147981\t0.862447\n",
    "### 1\tNumber of Stars\t-0.210578\t0.810116\n",
    "### 7\tNumber of Photometry Time Series\t-4.216848\t0.014745 *** \n",
    "### When log odds is less than 1, it means for every additional unit of that feature, the odds of detecting an exoplanet by transits decreases. In the case of \"Number of Photometry Time Series\" for example, the odds decrease by approximately 98.53% (1- 0.0147 = .9853)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d944b0a-cf3a-4c0b-aee1-60ed7b9e8ebd",
   "metadata": {},
   "source": [
    "# Deep Neural Network\n",
    "## the DNN ipynb starts off the same, importing the relevant libraries. Since I am incorporating a Deep Neural Network, I imported tensorflow for keras operations and optuna for hyperparameter optimization\n",
    "## Loaded and read the preprocessed exoplanet composite csv using pandas ; removed unimportant features according to the XGBoost model\n",
    "## Defined my targets and my features \n",
    "## Observed feature correlations and set the random seed 42 once again\n",
    "## Split the data following 80:10:10 (training: testing: validation)\n",
    "## Since I used tensorflow, I had to convert the dataframe into a np array since tensorflow models expect data in the form of a np array\n",
    "## Defined a function that returns the numpy array form of the parameter passed, and a list with all the data splits for x and y\n",
    "## Using list comprehension I called the converter function for each split in the list of data splits; the result is data splits of the proper type\n",
    "## Then I imported further: \n",
    "### Sequential helps actually build the DNN by grouping a linear stack of layers into a model\n",
    "### Dense is important for densely connected NN layers\n",
    "### Adam is the optimizer of choice\n",
    "### ReduceLROnPlateau helps optimize learning rate by lowering it once learning stops improving (LR scheduler)\n",
    "### TFKerasPruningCallback is vital for the actual training portion as it checks for pruning condition every epoch\n",
    "### KFold is for cross validation purposes\n",
    "## Once all the necessary tools are ready I began to build the DNN\n",
    "### First declared the input and output size; input layer is equal to the number of features (10, which can be accessed using thte shape of x_train which is in the format (rows, cols)) and output is equal to the number of predictions expected by the model (1) (There are 2 possible classifications in this binary problem ; 1 means detected by transits, 0 is not detected by transits but by another method, the DNN will ulimatetly calculate the probability the exoplanet it a case of class 0 or class 1 so it is essentially 1 output)\n",
    "### Defined a create_dnn function that takes trial number as a parameter (trial number is based on optuna) and its job is to create a new deep neural network for that trial number (each trial has a different combination of hyperparameters) \n",
    "#### The function defines the units (number of neurons) first and uses trial suggest int method to get an integer between the two bounds, in this case 10 and 100 with steps = 5 (integers can only be multiples of 5) and set that as the number of neurons for that trial\n",
    "#### Dropout rate is the same logic, but now trial suggests a float between 0.0 and 0.5 with steps 0.05 at a time. Dropout rate is the frequency at which neurons are randomly dropped during training \n",
    "#### Similarly, learning rate follows the same logic. The bounds are 1e-5 to 1e-2 with 0.05 as the step quantity. log scale is set to true for a better search\n",
    "### Next I actually built the DNN, using Sequential class\n",
    "#### First layer is the input layer naturally, so use keras.layers.Dense to create a densely connected layer and we put in the input_layer variable for the input_shape parameter. I used Rectified Linear Unit as the activation function because ReLu mitigates the vanishing gradient problem. The number of neurons is just the units variable\n",
    "#### Then I implement the dropout rate using keras.layers.Dropout\n",
    "#### I only created one hidden layer of neurons because 2 lowered the validation accuracy significantly ; 1 was enough to pick up the patterns and make generally valid predictions\n",
    "#### Following the Dropout line I created my output layer with sigmoid activation because sigmoid transforms the input into probabilities of 0 and 1, and has a clear threshold for decision making \n",
    "### After this I defined my objective functiton which takes the trial number as a parameter\n",
    "#### First thing to do is define kfold cross validation, using the imported KFold class with number of splits = 5 (the number of splits made on the data where each fold is used once as a validation set while the other 4 are used for training), shuffling as True and random_state as 42 (we created a seed for random state)\n",
    "#### Since I used a cross validation loop, I stored the validation accuracies in a list. The logic here was to take the average across all 5 folds as the cross validation accuracy. \n",
    "#### For each train index and validation index in the k-folded x train np arr, I defined the train fold and validatiton fold using these indices for both x and y\n",
    "#### Within the for loop, I called the create_dnn function with the corresponding trial number and stored it in the model variable. I used trial to suggest integers for both epochs (bounds 5 to 50 with steps quantity = 5) and batch size (bounds 5 to 30 with steps quantity = 5) \n",
    "#### Also used trial to suggest floats for patience (training periods the learning rate scheduler will wait before lowering the learning rate; helps avoid performance plateaus ; bounds 2 to 10 with steps quantity = 1) and factor (the factor at which the learning rate will be multiplied; bounds 0.1 to 0.5 with steps quantity = 0.1)\n",
    "#### Then I actually defined the learning rate scheduler variable, using the ReduceLROnPlateau with the metric we want to monitor being validation accuracy, patience and factor as defined above, and the minimum learning rate as 1e-7\n",
    "#### Following this I actually started the model training, fitting the created DNN and tossing in numpy arrays x train, y train, epochs, batch size and the validation data as parameters\n",
    "#### Incorporating TFKerasPruningCallBack checks on model progress and saves time and resources by stopping the training if the validation accuracy is too poor to continue \n",
    "#### PruningCallBack takes the trial number, the metric we want to monitor, and learning rate scheduler as parameters\n",
    "#### Verbose=2 just reduces output clutter\n",
    "#### After training, the validation accuracy values are stored in the training.history object, so we use the max() funcion to extract the greatest value and append this value to the list containing the validation accuracies for the current fold \n",
    "#### Once the loop is complete, we return the mean of the validation accuracies in the list\n",
    "### Then I used optuna to create a study (actually start the trials and hyperparameter optimization process) with the direction being to maximize validation accuracy\n",
    "#### Using the optimize method, tossing the objective function (which helps us optimize validation accuracy) and number of trials = 200 (enough considering the size of the composite dataset) training is commenced.\n",
    "### After all 200 trials I extracted the best trial value (the best validation accuracy achieved ~ 87.63%) and best trial parameters ({'units': 100,\n",
    "####  'dropout': 0.1,\n",
    "####  'learning_rate': 0.007511080260178754,\n",
    "####  'epochs': 50,\n",
    "####  'batch_size': 30,\n",
    "####  'patience': 6,\n",
    "####  'factor': 0.4})\n",
    "### These best values were stored in a best_params variable\n",
    "### With the new optimal hyperparameters, I proceeded to train a new DNN with these values\n",
    "### Essentially created the same structire but for each value I used the best_params variable with [parameter] as the key (ex: best_params['units']) and everything else (activation, input shape, output size, activation, optimizer, loss, metrics and LR scheduler) remained the same \n",
    "### Afterwards I finally fit the best model and used .evaluate method to analyze the test loss and test accuracy which came out to 0.4778 and .8463% respectively\n",
    "### Using the .predict method, I made the best DNN make actual predictions on the test set, and here is where the model outputs the probability for the binary class and converts it into a final prediction of 0 or 1 with a threshold of 0.5 (lets say 0.45 chance it is 0, 0.55 chance it is 1, the model interprets it as 1)\n",
    "### After deriving the probabilities, and converting them into integers, I imported evaluation metrics (classification report, accuracy score, roc_auc score, confusion matrix) and matplotlib for visual comprehension \n",
    "### This was the confusion matrix I got:\n",
    "### [[274  60]\n",
    "### [ 39 271]] \n",
    "### The DNN accuracy on the test set was 0.846, with an ROC AUC score of 93.57%, meaning if you randomly choose one instance of 0 and one instance of 1, there is a 93.57% chance the model will correctly identify which is which\n",
    "### Plotting the validation and training accuracy over the epochs, we generally see the trend we expect to see; As the epochs increase, the lines rise and stabilize near a certain accuracy, in this case is 0.846\n",
    "### Next I plotted the loss over the epochs; a primary goal of machine learning model training is to MINIMIZE loss over time; This is the trend we see as the first few epochs had extremely high loss, but the curve shoots down to a loss of almost 0 and behaves the way its expected with a good learning rate\n",
    "### Finally, I put the DNN feature importance on a table. After getting the weights from the input layer, calculating the importance of each feature by summing the absolute values of these weights across all neurons in the first hidden layer, which gives a measure of how much each feature contributes to the predictions made by the model, and normalizing, so that the features can be compared on a relative scale. I made a dataframe and put the values there. \n",
    "## By normalized weight, these are the top 4 features with the most influence on the model:\n",
    "### Feature\tNormalized Weight\n",
    "### 6\tNumber of Photometry Time Series\t0.252339\n",
    "### 4\tEcliptic Latitude [deg]\t0.216743\n",
    "### 5\tEcliptic Longitude [deg]\t0.168281\n",
    "### 3\tGalactic Longitude [deg]\t0.078873\n",
    "## Finally, I plotted the feature importance table on a graph for enhanced visual presentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc5066c-b091-4f4f-b372-679e937d84d9",
   "metadata": {},
   "source": [
    "# SUMMARY TABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f325ec1-dbf9-4a8b-9abb-852d7121af86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py310_ds)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
