{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eae61364-710e-414a-b853-15333763a724",
   "metadata": {},
   "source": [
    "# Concluding the Exoplanet Composite Discovery Method Predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43396f69-15e9-4760-98e0-e68067ff221b",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebbe627-e720-4a30-8fbe-63e5703574ed",
   "metadata": {},
   "source": [
    "### In the preprocessing ipynb, I started by using pandas to read the raw csv, made a copy and displayed the raw csv info\n",
    "### Using a missing value threshold of 0, I removed columns that exceeded this threshold. 0 made the most sense as a threshold to simplify model training and reduce problem complexity. \n",
    "### After reducing the number of features, I used the exoplanet archive column mapping csv to map the table name (ex: pl_name) to the Description (ex: Planet name) using a dictionary\n",
    "### Used a for loop through the number of code columns (358) and their definitions, where if row i in the database column matches a column name in the filtered raw data, you assign the key and value to the dictionary, removing trailing and leading whitespaces\n",
    "### Used boolean to confirm we mapped all database column names to their definitions\n",
    "### Then just reassigned the raw data column values (the names) as the values of the dictionary {key:value}\n",
    "### Checkpoint where we create a copy of the current dataset we are working with\n",
    "### Then I checked unique values in the \"Discovery Method\" column in the dataset to get an idea of the discovery methods possible\n",
    "### Using value_count, I got an idea of the frequency of each of the discovery methods in the column. Transit was by far the most frequent\n",
    "### Some of the discovery methods only had 10 instances, so I used SMOTE (synthetic minority oversampling technique) to synthetically generate instances of the minority class for a better class balance. This is essential for machine learning models to properly learn how to predict the discovery methods and not just predict the most frequent discovery method. \n",
    "### Realized that if I encoded Discovery Facility, Discovery Telescope and Discovery Instrument, there are 70-90 unique values for EACH, so encoding these was determined unfeasable\n",
    "### So I dropped them along with Planet name and Host name, which would be irrelevant to model training\n",
    "### Then I removed all the one hot binary encoded variables \"Detected by...\" columns except for transits\n",
    "### This is because if \"Detected by Transits\" is 0, we already know that it has to be one of the other 10 discovery methods\n",
    "### Before doing that, I removed instances of exoplanets that were found by more than one discovery method for more straightforward model training\n",
    "### This was done by summmating each of the target dummies for each of the exoplanets, and if the sum was greater than 1 that means it was discovered by more than one method, warranting its removal\n",
    "### After checking the new observation count for each of the \"Detected by...\" methods, we dropped the unnecessary \"Detected by...\" columns\n",
    "### Checking the ratio of \"Detected by transits\" to the total number of instances, I saw it transits was roughly 71% of discovery method instances\n",
    "### Imported SMOTE and specified my features (all variables except \"Detected by Transits\") and targets (\"Detected by Transits\")\n",
    "### The new ratio was 50%, implying SMOTE worked as intended (50% exoplanets discovered by Transits, 50% were not)\n",
    "### Then proceeded with standardization; Removed columns that could not be standardized (Discovery Year, Circumbinary Flag, Controversial Flag) and added these after standardizing\n",
    "### Imported StandardScaler function from sklearn preprocessing library and fit the scaler to all the unscaled features.\n",
    "### Then applied scaler.transform to all the unscaled features besides the ones excluded above. Checked the shapes of features, targets and the variables we removed, they all must match\n",
    "### Since we used StandardScaler the scaled features are now in numpy array format. To combine it with the other excluded variables, it must first be convered to a pandas dataframe. Then we just added the excluded variables to the new pandas dataframe of the scaled features\n",
    "### Converted discovery year to be treated as a categorical variable for the purposes of this project\n",
    "### One last checkpoint (creating copy of the current dataframe we are working with) \n",
    "### Finally export the preprocessed dataframe and turn it into a csv to be saved in the current directory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py310_ds)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
