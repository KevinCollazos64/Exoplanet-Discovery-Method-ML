{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eae61364-710e-414a-b853-15333763a724",
   "metadata": {},
   "source": [
    "# Concluding the Exoplanet Composite Discovery Method Predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43396f69-15e9-4760-98e0-e68067ff221b",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebbe627-e720-4a30-8fbe-63e5703574ed",
   "metadata": {},
   "source": [
    "### In the preprocessing ipynb, I started by using pandas to read the raw csv, made a copy and displayed the raw csv info\n",
    "### Using a missing value threshold of 0, I removed columns that exceeded this threshold. 0 made the most sense as a threshold to simplify model training and reduce problem complexity. \n",
    "### After reducing the number of features, I used the exoplanet archive column mapping csv to map the table name (ex: pl_name) to the Description (ex: Planet name) using a dictionary\n",
    "### Used a for loop through the number of code columns (358) and their definitions, where if row i in the database column matches a column name in the filtered raw data, you assign the key and value to the dictionary, removing trailing and leading whitespaces\n",
    "### Used boolean to confirm we mapped all database column names to their definitions\n",
    "### Then just reassigned the raw data column values (the names) as the values of the dictionary {key:value}\n",
    "### Checkpoint where we create a copy of the current dataset we are working with\n",
    "### Then I checked unique values in the \"Discovery Method\" column in the dataset to get an idea of the discovery methods possible\n",
    "### Using value_count, I got an idea of the frequency of each of the discovery methods in the column. Transit was by far the most frequent\n",
    "### Some of the discovery methods only had 10 instances, so I used SMOTE (synthetic minority oversampling technique) to synthetically generate instances of the minority class for a better class balance. This is essential for machine learning models to properly learn how to predict the discovery methods and not just predict the most frequent discovery method. \n",
    "### Realized that if I encoded Discovery Facility, Discovery Telescope and Discovery Instrument, there are 70-90 unique values for EACH, so encoding these was determined unfeasable\n",
    "### So I dropped them along with Planet name and Host name, which would be irrelevant to model training\n",
    "### Then I removed all the one hot binary encoded variables \"Detected by...\" columns except for transits\n",
    "### This is because if \"Detected by Transits\" is 0, we already know that it has to be one of the other 10 discovery methods\n",
    "### Before doing that, I removed instances of exoplanets that were found by more than one discovery method for more straightforward model training\n",
    "### This was done by summmating each of the target dummies for each of the exoplanets, and if the sum was greater than 1 that means it was discovered by more than one method, warranting its removal\n",
    "### After checking the new observation count for each of the \"Detected by...\" methods, we dropped the unnecessary \"Detected by...\" columns\n",
    "### Checking the ratio of \"Detected by transits\" to the total number of instances, I saw it transits was roughly 71% of discovery method instances\n",
    "### Imported SMOTE and specified my features (all variables except \"Detected by Transits\") and targets (\"Detected by Transits\")\n",
    "### The new ratio was 50%, implying SMOTE worked as intended (50% exoplanets discovered by Transits, 50% were not)\n",
    "### Then proceeded with standardization; Removed columns that could not be standardized (Discovery Year, Circumbinary Flag, Controversial Flag) and added these after standardizing\n",
    "### Imported StandardScaler function from sklearn preprocessing library and fit the scaler to all the unscaled features.\n",
    "### Then applied scaler.transform to all the unscaled features besides the ones excluded above. Checked the shapes of features, targets and the variables we removed, they all must match\n",
    "### Since we used StandardScaler the scaled features are now in numpy array format. To combine it with the other excluded variables, it must first be convered to a pandas dataframe. Then we just added the excluded variables to the new pandas dataframe of the scaled features\n",
    "### Converted discovery year to be treated as a categorical variable for the purposes of this project\n",
    "### One last checkpoint (creating copy of the current dataframe we are working with) \n",
    "### Finally export the preprocessed dataframe and turn it into a csv to be saved in the current directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d106126e-e636-4d80-9730-15444fda80e4",
   "metadata": {},
   "source": [
    "## XGB Boost ALL Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee90ffc3-a515-4777-b6f9-b0421fb128df",
   "metadata": {},
   "source": [
    "### Why XGB?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb04f86d-a73e-4e97-80ac-6df25b8873d2",
   "metadata": {},
   "source": [
    "### After the initial preprocessing I got started with the first model I deployed for this analysis\n",
    "### XGBoost was a great first option because it is a powerful implementation of gradient boosting algorithms designed for tabular data\n",
    "#### The XGB captures complex patterns in data by combining predictions of multiple weak learners (typically decision trees)\n",
    "### XGB also provides clear metrics for feature importance, allowing enhanced understanding of which features most influences the model's predictions. This is useful to my task in predicting exoplanet discovery methods and finding out which features influenced the model to make these predictions \n",
    "### Additionally, XGB handles unbalanced data well. Although we addressed these imbalances in the preprocessing, it is still beneficial that XGB has several techniques to handle imbalanced data, such as scale_pos_weight to assign more weight to the less frequent classes to improve the model's ability to predict them\n",
    "### XGB is also very fast and high performing, as it handles sparse data, uses parallel processing and regularization techniques that prevent overfitting \n",
    "### XGB has flexibility with the feature types, including categorical and continuous features, without needing to extensively preprocess\n",
    "### Finally, XGBoost has been widely adopted in scientific fields like astronomy for its accuracy and robustness in both classification and regression tasks. It is very handy in managing non-linear relationships and interactions between features "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72727544-03f6-4ad6-9545-df428055b916",
   "metadata": {},
   "source": [
    "### By including all the features in this XGB model, I aimed to reduce the dimensionality of the problem by identifying which features weighed the most according to the model\n",
    "### Firstly imported the preprocessed data using pandas, and defined my train test split, as well as features and targets (features are all features except the target variable we are predicting, which is detected by transits)\n",
    "### Then, I observed feature correlations to observe if there was any moderate to high correlation between variables, which would interfere with model training and performance. Should I have found any notable correlations, that warranted removal of one of them\n",
    "### After this I proceeded to set a random seed for reproducability using random and numpy libraries. This means every time I set random state to 42 it shuffles the same RANDOM way\n",
    "### Then I actually split the data into training, testing and validation (80% training, 10% testing, 10% validation). This was achieved through 2 separate splits (one to get 80:20, then to get that 20 into 10:10)\n",
    "### Following the split, I used the XGBClassifier class to create my model from the xgb library, with logloss as the evaluation metric. logloss is appropriate because it is very suitable for binary classification problems\n",
    "### Since XGB cannot have \"[\" or \"]\" or \"<\" or \">\" characters in the feature names, I used the lambda function to replace these characters with empty spaces\n",
    "### Then I actually fit the model with the new clean training, and validation data, with early stopping = 10 so that if theres no improvement in 10 consecutive trials the model stops to prevent overfitting and to make the overall process more efficient\n",
    "### After fitting the model, I used .predict method on the cleaned test set, so I can see what predictions the model makes on each instance with my own eyes\n",
    "### Evaluation of performance metrics was done using classificaiton report, accuracy score, roc auc score, and a confusion matrix. The actual evaluation was a comparison between the model's predictions and the y_test (the actual values of whether or not this exoplanet was discovered by transits)\n",
    "### The XGB Model with all features had an accuracy of a whopping 96.42% on the test set, with an ROC AUC of 98.59%\n",
    "### Right after the evaluation metrics, I looked at the K fold cross validation score using sklearn the library, and declared 5 shuffled folds. I calculated the average cross validation accuracy (96.77%) and the average cross validation ROC AUC (99.3%)\n",
    "### I wanted to see which features were deemed important by the model so I made a feature importance table, where the importance type is the WEIGHT of the feature. Naturally this means that I wanted to see which features had the greater weight on the model. \n",
    "### After extracting the weights of each of the coefficients from the fitted xgboost model using .get_booster().get_score('weight'), and the feature names from the feature training data, I created a pandas dataframe with the Features and their corresponding importance \n",
    "### Firstly had to turn the feature_imporance items into a list, set the columns, and sorted the values based on Importance in descending order\n",
    "### I then plotted this feature importance table using matplot lib, and used xgb.plot_importance to directly plot from the xgb model \n",
    "### This plot demonstrates the F score. A higher F score indicates the feature is deemed important by the model\n",
    "### For the rest of my models, to reduce dimensionality, I only included features that had an F score of 30 or above. These being: \n",
    "### 4   Ecliptic Latitude deg\t271.0\n",
    "### 2\tGalactic Latitude deg\t250.0\n",
    "### 3\tGalactic Longitude deg\t223.0\n",
    "### 5\tEcliptic Longitude deg\t201.0\n",
    "### 11\tDiscovery Year\t132.0\n",
    "### 1\tNumber of Planets\t78.0\n",
    "### 6\tNumber of Photometry Time Series\t32.0\n",
    "### 0\tNumber of Stars\t20.0\n",
    "\n",
    "### With this new revised list of features, I had a better idea of the features I wanted to train the rest of my models with"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ff3d73-5b7f-4f12-b2bb-a76ee091de48",
   "metadata": {},
   "source": [
    "# XGBoost REFINED features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adaab5f-19ec-4cd5-b239-7202ca8698a8",
   "metadata": {},
   "source": [
    "## After training and testing the XGBoost with all features, I trained and tested a new XGBoost model with refined features. The purpose of this is to see if after reducing multi-dimensionality, model performance would be enhaced as the problem becomes less complex. \n",
    "## The first time around training and testing the new XGBoost, I had excluded \"Number of Stars,\" but found that model performance suffered every so slightly (test accuracy went down 0.3% from 96.42% -> 96.11%), ultimately prompting me to leave it in. \n",
    "## The ipynb starts off the same, using pandas to import and read the preprocessed composite csv\n",
    "## Then, I created a list full of the features I was going to exclude, and used the .drop method to remove them with axis=1 (1 is for columns, 0 is for rows. Makes sense because format is (rows, cols))\n",
    "## After this I defined my targets variable (detected by transits) aka the variable I want my model to predict, while features are everything else (did this by just dropping the targets variable)\n",
    "## Feature analysis includes looking at the correlations so I just used the .corr() method to get a table looking at correlationss between variables. For model training its rule of thumb to avoid multicollinearity. \n",
    "## I observed the following correlations:\n",
    "## Weak:\n",
    "### Ecliptic Latitude and Number of Photometry Time Series; -0.222\n",
    "### Number of Photometry Time Series and Discovery Year; -0.253\n",
    "\n",
    "\n",
    "## Moderate:\n",
    "### Galactic Latitude and Ecliptic Latitude; .463\n",
    "### Galactic Longitude and Ecliptic Latitude; -.657\n",
    "### Galactic Longitude and Ecliptic Longitude; -.408\n",
    "### Ecliptic Latitude and Ecliptic Longitude; .498\n",
    "\n",
    "\n",
    "## The rest of the feature pairs can be characterized as having very weak correlations\n",
    "\n",
    "## Since none of the feature correlations were excessive, I didn't remove any further variables\n",
    "## Following observing the feature correlations I set the random seed for 42 once again\n",
    "## I split the data into training, testing and validation in the same exact way as in the XGBoost model with all features (80:10:10)\n",
    "## The XGBoost was created essentially in the same way as in the ipynb with all features. logloss is still the best evaluation metric for this problem\n",
    "## Similar to the XGBoost with all features, removed incompatible characters from the features for training, testing and validation\n",
    "## Then fit the new XGBoost with early stopping rounds = 10 again\n",
    "## After fitting, I wanted to see the actual predictions of the model on the test set again, so i did this with .predict method on x_test_clean\n",
    "## Looked at the same performance metrics (classification report, accuracy score, roc auc score and confusion matrix) \n",
    "## The Refined XGboost model had the same accuracy on the test set: 96.42%, with a lower ROC of 98.57%. The reduction is so miniscule it can be considered insignificant. \n",
    "## The confusion matrices in both were also exactly the same\n",
    "## Same protocol for k-cross validation, but this time the refined model performed a bit worse at 96.67% and a mean CV ROC AUC of 99.25%, both reductions in performance insignificant. \n",
    "## The feature importance table looks exactly the same as in the XGBoost with all features, just with different F scores. Both feature tables had more or less the same order and importance ranking for the features, the only difference being galactic longitude scoring higher than galactic longitude in the XGBoost refined model. Again, this difference (2) is insignificant. \n",
    "## Both XGB models suggest that positional data (ecliptic and galactic coordinates) plays a critical role in detecting exoplanets by the transit method likely due to the necessity of specific orbital alignments. Discovery year highlights the importance of technological advancements in improving the detection capabilities of the transit method over time. \n",
    "\n",
    "### Ecliptic Latitude (deg)\n",
    "#### Ecliptic latitude is the position of the exoplanet relative to the ecliptic plane, which is the apparent path of the Sun on the celestial sphere. A high ranking of importance for feature suggests that the position of an exoplanet relative to this plane is crucial for detecting it via transits. This makes sense because transit detection is easier when the orbital plane of the exoplanet is aligned with the line of sight from Earth, which is more likely near the ecliptic plane.\n",
    "\n",
    "### Galactic Longitude (deg)\n",
    "#### Galactic longitude refers to the position of the exoplanet within the Milky Way. This feature's high importance indicatess that certain regions of the galaxy are more conducive to detecting exoplanets by transits, possibly due to the density of stars or observational strategies that focus on specific regions of the galaxy.\n",
    "\n",
    "### Galactic Latitude (deg)\n",
    "#### Similar to galactic longitude, galactic latitude describes the exoplanet's position relative to the galactic plane. The importance of this feature suggests that exoplanet detection via transits might be more successful in particular regions of the galaxy, such as near the galactic plane where star density is higher, thus increasing the likelihood of observing a transit.\n",
    "\n",
    "### Ecliptic Longitude (deg)\n",
    "#### Ecliptic longitude, like ecliptic latitude, is another coordinate that describes the position of the exoplanet in the sky. Its importance in the model implies that the specific positioning relative to the ecliptic plane has a significant impact on the ability to detect transits.\n",
    "\n",
    "### Discovery Year \n",
    "#### The discovery year being an important feature reflects the improvements in technology and observational techniques over time. As methods for detecting transits have advanced, the ability to discover exoplanets using this method has likely improved with it. \n",
    "\n",
    "### Context:\n",
    "#### Exoplanet Detection by Transits: The transit method relies on detecting the slight dimming of a star as an exoplanet passes in front of it. The effectiveness of this method is influenced by the alignment of the exoplanet's orbit with the observer's line of sight, which is why positional coordinates like ecliptic and galactic latitude/longitude are crucial.\n",
    "\n",
    "#### Technological Advances: The importance of \"Discovery Year\" suggests that as detection technology has improved, particularly with missions like Kepler and TESS, the ability to detect exoplanets using the transit method has also increased. This makes sense because of historical trends in exoplanet discovery, where a surge in transit detections followed the launch of these missions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a3fd69-3db0-4e93-82f3-500b08f0850f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py310_ds)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
