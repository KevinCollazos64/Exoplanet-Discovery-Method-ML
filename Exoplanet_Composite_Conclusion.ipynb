{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eae61364-710e-414a-b853-15333763a724",
   "metadata": {},
   "source": [
    "# Concluding the Exoplanet Composite Discovery Method Predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43396f69-15e9-4760-98e0-e68067ff221b",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebbe627-e720-4a30-8fbe-63e5703574ed",
   "metadata": {},
   "source": [
    "### In the preprocessing ipynb, I started by using pandas to read the raw csv, made a copy and displayed the raw csv info\n",
    "### Using a missing value threshold of 0, I removed columns that exceeded this threshold. 0 made the most sense as a threshold to simplify model training and reduce problem complexity. \n",
    "### After reducing the number of features, I used the exoplanet archive column mapping csv to map the table name (ex: pl_name) to the Description (ex: Planet name) using a dictionary\n",
    "### Used a for loop through the number of code columns (358) and their definitions, where if row i in the database column matches a column name in the filtered raw data, you assign the key and value to the dictionary, removing trailing and leading whitespaces\n",
    "### Used boolean to confirm we mapped all database column names to their definitions\n",
    "### Then just reassigned the raw data column values (the names) as the values of the dictionary {key:value}\n",
    "### Checkpoint where we create a copy of the current dataset we are working with\n",
    "### Then I checked unique values in the \"Discovery Method\" column in the dataset to get an idea of the discovery methods possible\n",
    "### Using value_count, I got an idea of the frequency of each of the discovery methods in the column. Transit was by far the most frequent\n",
    "### Some of the discovery methods only had 10 instances, so I used SMOTE (synthetic minority oversampling technique) to synthetically generate instances of the minority class for a better class balance. This is essential for machine learning models to properly learn how to predict the discovery methods and not just predict the most frequent discovery method. \n",
    "### Realized that if I encoded Discovery Facility, Discovery Telescope and Discovery Instrument, there are 70-90 unique values for EACH, so encoding these was determined unfeasable\n",
    "### So I dropped them along with Planet name and Host name, which would be irrelevant to model training\n",
    "### Then I removed all the one hot binary encoded variables \"Detected by...\" columns except for transits\n",
    "### This is because if \"Detected by Transits\" is 0, we already know that it has to be one of the other 10 discovery methods\n",
    "### Before doing that, I removed instances of exoplanets that were found by more than one discovery method for more straightforward model training\n",
    "### This was done by summmating each of the target dummies for each of the exoplanets, and if the sum was greater than 1 that means it was discovered by more than one method, warranting its removal\n",
    "### After checking the new observation count for each of the \"Detected by...\" methods, we dropped the unnecessary \"Detected by...\" columns\n",
    "### Checking the ratio of \"Detected by transits\" to the total number of instances, I saw it transits was roughly 71% of discovery method instances\n",
    "### Imported SMOTE and specified my features (all variables except \"Detected by Transits\") and targets (\"Detected by Transits\")\n",
    "### The new ratio was 50%, implying SMOTE worked as intended (50% exoplanets discovered by Transits, 50% were not)\n",
    "### Then proceeded with standardization; Removed columns that could not be standardized (Discovery Year, Circumbinary Flag, Controversial Flag) and added these after standardizing\n",
    "### Imported StandardScaler function from sklearn preprocessing library and fit the scaler to all the unscaled features.\n",
    "### Then applied scaler.transform to all the unscaled features besides the ones excluded above. Checked the shapes of features, targets and the variables we removed, they all must match\n",
    "### Since we used StandardScaler the scaled features are now in numpy array format. To combine it with the other excluded variables, it must first be convered to a pandas dataframe. Then we just added the excluded variables to the new pandas dataframe of the scaled features\n",
    "### Converted discovery year to be treated as a categorical variable for the purposes of this project\n",
    "### One last checkpoint (creating copy of the current dataframe we are working with) \n",
    "### Finally export the preprocessed dataframe and turn it into a csv to be saved in the current directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d106126e-e636-4d80-9730-15444fda80e4",
   "metadata": {},
   "source": [
    "## XGB Boost ALL Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee90ffc3-a515-4777-b6f9-b0421fb128df",
   "metadata": {},
   "source": [
    "### Why XGB?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb04f86d-a73e-4e97-80ac-6df25b8873d2",
   "metadata": {},
   "source": [
    "### After the initial preprocessing I got started with the first model I deployed for this analysis\n",
    "### XGBoost was a great first option because it is a powerful implementation of gradient boosting algorithms designed for tabular data\n",
    "#### The XGB captures complex patterns in data by combining predictions of multiple weak learners (typically decision trees)\n",
    "### XGB also provides clear metrics for feature importance, allowing enhanced understanding of which features most influences the model's predictions. This is useful to my task in predicting exoplanet discovery methods and finding out which features influenced the model to make these predictions \n",
    "### Additionally, XGB handles unbalanced data well. Although we addressed these imbalances in the preprocessing, it is still beneficial that XGB has several techniques to handle imbalanced data, such as scale_pos_weight to assign more weight to the less frequent classes to improve the model's ability to predict them\n",
    "### XGB is also very fast and high performing, as it handles sparse data, uses parallel processing and regularization techniques that prevent overfitting \n",
    "### XGB has flexibility with the feature types, including categorical and continuous features, without needing to extensively preprocess\n",
    "### Finally, XGBoost has been widely adopted in scientific fields like astronomy for its accuracy and robustness in both classification and regression tasks. It is very handy in managing non-linear relationships and interactions between features "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72727544-03f6-4ad6-9545-df428055b916",
   "metadata": {},
   "source": [
    "### By including all the features in this XGB model, I aimed to reduce the dimensionality of the problem by identifying which features weighed the most according to the model\n",
    "### Firstly imported the preprocessed data using pandas, and defined my train test split, as well as features and targets (features are all features except the target variable we are predicting, which is detected by transits)\n",
    "### Then, I observed feature correlations to observe if there was any moderate to high correlation between variables, which would interfere with model training and performance. Should I have found any notable correlations, that warranted removal of one of them\n",
    "### After this I proceeded to set a random seed for reproducability using random and numpy libraries. This means every time I set random state to 42 it shuffles the same RANDOM way\n",
    "### Then I actually split the data into training, testing and validation (80% training, 10% testing, 10% validation). This was achieved through 2 separate splits (one to get 80:20, then to get that 20 into 10:10)\n",
    "### Following the split, I used the XGBClassifier class to create my model from the xgb library, with logloss as the evaluation metric. logloss is appropriate because it is very suitable for binary classification problems\n",
    "### Since XGB cannot have \"[\" or \"]\" or \"<\" or \">\" characters in the feature names, I used the lambda function to replace these characters with empty spaces\n",
    "### Then I actually fit the model with the new clean training, and validation data, with early stopping = 10 so that if theres no improvement in 10 consecutive trials the model stops to prevent overfitting and to make the overall process more efficient\n",
    "### After fitting the model, I used .predict method on the cleaned test set, so I can see what predictions the model makes on each instance with my own eyes\n",
    "### Evaluation of performance metrics was done using classificaiton report, accuracy score, roc auc score, and a confusion matrix. The actual evaluation was a comparison between the model's predictions and the y_test (the actual values of whether or not this exoplanet was discovered by transits)\n",
    "### The XGB Model with all features had an accuracy of a whopping 96.42% on the test set, with an ROC AUC of 96.46\n",
    "### Right after the evaluation metrics, I looked at the K fold cross validation score using sklearn the library, and declared 5 shuffled folds. I calculated the average cross validation accuracy (96.77%) and the average cross validation ROC AUC (99.3%)\n",
    "### I wanted to see which features were deemed important by the model so I made a feature importance table, where the importance type is the WEIGHT of the feature. Naturally this means that I wanted to see which features had the greater weight on the model. \n",
    "### After extracting the weights of each of the coefficients from the fitted xgboost model using .get_booster().get_score('weight'), and the feature names from the feature training data, I created a pandas dataframe with the Features and their corresponding importance \n",
    "### Firstly had to turn the feature_imporance items into a list, set the columns, and sorted the values based on Importance in descending order\n",
    "### I then plotted this feature importance table using matplot lib, and used xgb.plot_importance to directly plot from the xgb model \n",
    "### This plot demonstrates the F score. A higher F score indicates the feature is deemed important by the model\n",
    "### For the rest of my models, to reduce dimensionality, I only included features that had an F score of 30 or above. These being: \n",
    "### 4   Ecliptic Latitude deg\t271.0\n",
    "### 2\tGalactic Latitude deg\t250.0\n",
    "### 3\tGalactic Longitude deg\t223.0\n",
    "### 5\tEcliptic Longitude deg\t201.0\n",
    "### 11\tDiscovery Year\t132.0\n",
    "### 1\tNumber of Planets\t78.0\n",
    "### 6\tNumber of Photometry Time Series\t32.0\n",
    "\n",
    "### With this new revised list of features, I had a better idea of the features I wanted to train the rest of my models with"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py310_ds)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
